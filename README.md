# multilayer-perceptron

Implementation of a multilayer perceptron containing an input layer, hidden layer, and output layer. 

The weights are initialised using the He initialisation method. The hidden and output layers use the ReLU and softmax activation functions respectively. The gradients of the (cross-entropy) loss function with respect to the weights and biases are calculated and the parameters are updated via batch gradient descent. 

A demonstration of the model being trained on data from the MNIST database is provided.

<img src="https://github.com/user-attachments/assets/a6be99b3-a6b0-4fcd-bec8-bb2f3a865f2e" alt="demonstration" width="600" />
